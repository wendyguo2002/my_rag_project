# Behavioral Stories

## 1. HoloLens — New Teammate Onboarding / Speaking Up

During a dense code review on the HoloLens project, I noticed a new teammate was silent and flipping between files. I looked over and realized she probably had no idea what was going on, since she hadn't even seen the full system yet. I remembered how overwhelmed I felt when I first joined the team. Instead of pushing through the discussion, I asked the group for a short reset: why we were changing the module, what "good" would look like, and how we would test it. That two-minute context check changed the tone. The teammate jumped in with a couple of sharp questions about edge cases, we aligned on a simpler validation plan, and the review wrapped on time with concrete next steps. I followed up with a short summary so the teammate could ramp smoothly for the next step.

## 2. ProofBlocks — Two Professors, Two Directions → You Prototyped Both

My team, including me and two professors, had different opinions on how to present a ProofBlocks feature called ordering feedback. One faculty member favored a flexible design with many feedback options for instructors to choose, but another pushed for a minimal UI so it's more straightforward for instructors to understand and use that feature. Instead of choosing one direction over the other, I proposed building two demo versions — one flexible, one simple — so we could test both and see what made the most sense. I posted the idea in a GitHub thread where we'd been discussing the feature, and that actually sparked a much broader conversation. Other contributors jumped in, shared their perspectives, and it turned into a collaborative design effort that I didn't expect at first. Especially when it came to the user interface, testing helped us see what people actually understood and found useful. In the end, we leaned more toward the simpler version because it just worked better for most users. So I learned that sometimes, even if a system can be made more powerful, simpler can still be the better design choice if it leads to a cleaner user experience.

## 3. ProofBlocks — Feature Shipped and Adopted in Course

During my 2024 summer research on ProofBlocks, I noticed that the tool only gave generic messages when students submitted incorrect proof block orderings. I realized instructors needed a way to give more personalized, targeted feedback. I set out to design and implement a feature where instructors could tag specific incorrect logical steps with their own feedback. This required adding functionality that hadn't existed in the original tool. I modified the core checking logic, edited the tagging system, and opened a pull request with documentation and examples. The PR sparked a wide GitHub discussion — Prof. Poulsen reviewed the design, while others gave input on performance, naming, and usability. The feature was merged and used in the Fall 2024 course, helping students better understand their mistakes. It also became one of my proudest accomplishments, showing me how impactful collaborative development and clear communication can be.

## 4. Time Management — Overlapping CS Projects Plus an Exam

One semester, I took two demanding CS core courses that had overlapping project deadlines almost every cycle. These projects were time-consuming, with long test cases — one even took over an hour to run — and I also had an econ exam that week. I needed to stay ahead of both projects consistently throughout the semester, while still preparing for exams and making good use of office hours before they got crowded. I began each project the day it was released and used office hour schedules to prioritize my work. During long test runs, I would multitask by reviewing econ lectures and typing debug notes. Since I often progressed faster than classmates, I couldn't rely much on peer help — so I used office hours heavily. But I'm still willing to help classmates who were stuck — even just by sharing general strategies or reminding them about common pitfalls. What I took away from that semester was the importance of working proactively, not reactively — by building systems, using resources smartly, and adapting my pace week by week, I turned a tough schedule into a manageable routine.

## 5. Under Pressure — Unexpected Things Happened

Two days before our final game design showcase, my teammate's laptop crashed, and we lost the latest version of our project — which included critical merged data structures and parameters. Since we were in the final integration phase, this setback put the entire demo at risk. I had to help the team reconstruct the missing code, re-merge everything, and ensure the game was debugged and functional in time for the demo. We also needed to investigate and fix any new issues that might've emerged during the recovery. I led the recovery by recalling recent edits with my teammates and manually re-implementing some core functionalities (cooking system) from memory and backup files. During testing, I identified and fixed a new bug in the ingredient combination logic that we haven't noticed before and streamlined the user flow to make it more intuitive. We successfully recovered our game and delivered a smooth demo that was well-received for its intuitive design. I learned how crucial version control is under pressure and how staying calm, prioritizing clearly, and supporting your team can turn a crisis into a success.

## 6. Failure: Overcomplicating the Cooking System

In the game design class, I was in charge of building the cooking system, and I wanted it to be as flexible and deep as possible. I designed it so the order of ingredients, and even the timing all affected the final result. My goal was to create a highly customizable system that gave players lots of creative possibilities. But I spent most of my time on complex data structures and logic, and didn't think enough about how intuitive the system would be for players. During our first playtest, things went wrong. Players didn't understand how to use the system, the UI wasn't intuitive, and many of them expected a drag-and-drop, while my design used click-based interactions. I got a lot of feedback suggesting that the system was too complicated and lacked proper instruction. It was tough deleting a system I had spent days building, but I quickly regrouped with my team, redesigned the system to be simpler, and focused on making it more intuitive and testable. The updated version worked much better in testing, and our feedback scores improved. I learned that no matter how clever the backend is, if users can't understand or enjoy it, the system fails — so now I prioritize clarity, user testing, and feedback much earlier in the design process.

## 7. Learn Things Quickly

Our team was tasked with verifying that a newly rewritten HoloLens data processing pipeline fully replicated the logic of an older version created by a not CS major phd student. We had just one week to finish the comparison before a demo for visiting middle school students. We needed to understand both codebases, identify mismatches in logic and data structures, and correct any inconsistencies. I took on the responsibility of noting structural differences and tracing logic between the two versions. I read through both versions of the code and kept detailed notes on data structures, helper functions, and any logic that seemed mismatched. As the logic got more complex, I suggested we review the old version by back-tracing variables from the outputs to the source logic. We went layer by layer, matching variables and rewriting parts of the new version where no direct equivalent existed. At one point, we were stuck for hours on a strange bug: after fixing logic in Step 3, the output from Step 2 suddenly became incorrect—even though we had already verified it earlier. We added print statements and reviewed the logic multiple times, but nothing made sense. That's when I remembered a tricky bug I ran into last semester involving shallow copy vs. deep copy. I suggested we check for that, and we found one line where a list was being passed by reference without a proper copy. That small oversight caused earlier results to get overwritten. Thanks to my notes and past debugging experience, we fixed a critical logic error that confused the team and would've been hard to spot otherwise. We completed the comparison on time and ensured accurate outputs for the demo.

## 8. Different Background

In my game design course, our team split features early—mine was the ingredient backpack and Alex handled weapons. As mechanics evolved, our code overlapped around shared systems like inventory. My backpack auto-updated counts on pick-up/use/combine, and Alex wanted the same for bullets. When I shared my functions, they failed in his code because our data structures were different and he lacked some parameters my logic required. We sat down, compared structures, and agreed not to force a rewrite. Instead, I abstracted the interface and built small wrapper helpers he could call with the variables he already had. We focused only on the connection points: what data to share, how to trigger updates, and how to avoid duplicate logic. Integration went smoothly without retraining anyone on the other's internals or redesigning everything. The takeaway: in evolving team projects, you don't need everyone to master every module; you need clear interfaces and mutual respect. That keeps velocity high while producing a flexible system that fits different working styles.

## 9. Data Integrity & False-Positive Control (Reconciliation Mindset)

In noisy field environments our simple thresholding was flooding the HoloLens overlay with false positives. It slowed operators and eroded trust. My goal was to raise valid-signal detection and precision without missing real events, and still keep the experience real-time. I replaced the thresholding with a feed-forward neural network and engineered 15+ temporal-correlation features—things like rise time, peak-amplitude ratios, and pulse width—to separate signal from noise. I added Bayesian uncertainty calibration so low-confidence outputs wouldn't pollute the overlay. Then I compared the model's decisions against our prior threshold baseline on representative recordings to confirm we were consistent where it mattered and better where noise dominated. Precision improved to 91% from 76%, signal-to-noise ratio increased 18%, and valid signal detection rose from 89% to 94% mainly by cutting false positives—so operators saw cleaner, more trustworthy overlays at real-time speed.
